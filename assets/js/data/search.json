[ { "title": "Building a WordPress Cluster with Terraform", "url": "/posts/building-a-wordpress-cluster-with-terraform/", "categories": "", "tags": "", "date": "2022-02-02 19:21:00 -0300", "snippet": "Building a WordPress Cluster - Part 01 (Networking)using Terraform to deploy: an RDS database; an EKS Cluster; an Amazon EFS file system a VPC a Route Table an Internet Gateway […] perhaps a bit more.Configuring the local environmentWe’ll be using the Layer Style Guide, but perhaps a bit reduced, due to the scope of this project.This will be our starting point:➜ WordPress-RDS-EKS-Cluster tree .├── k8s└── terraform └── layers ├── computing ├── efs └── networkingI’d like to start with the basics - networking. As we’ll use a managed database the setup will be a tad simpler. However, in this guide we’ll use Invalid CIDRs for our pod network. One can see a few details on how it works by heading to this guide.Other than the enviroment variables correctly set (so we can run terraform plan and terraform apply), there will be no shortcuts.In our network, we’ll need: A VPC At least one subnet (but we’ll use two).Create four files, to begin with on networking folder - provider.tf, subnet.tf, vpc.tf, variables.tf. That way, we can keep everything in its place.Content of provider.tf:terraform { required_providers { aws = { source = &quot;hashicorp/aws&quot; version = &quot;~&amp;gt; 3.74.0&quot; # Current Latest } }}provider &quot;aws&quot; { region = &quot;us-east-1&quot;}Then, run terraform init from your terminal.We’ll use a /16 for our main subnet, so as for our secondary (invalid) one.This is the end result of the vpc.tf file:resource &quot;aws_vpc&quot; &quot;main_vpc&quot; { cidr_block = &quot;10.0.0.0/16&quot; tags = { Name = &quot;default-vpc&quot;, ManagedBy = &quot;Terraform&quot; }}resource &quot;aws_vpc_ipv4_cidr_block_association&quot; &quot;invalid_cidr&quot; { vpc_id = aws_vpc.main_vpc.id cidr_block = &quot;100.64.0.0/16&quot;}This is mainly what we’ve been wondering about - a VPC with a big CIDR and an ipv4 cidr association to handle our secondary block, full of invalid IPs for our Pods.Now, we need to define which subnet is assigned to which AZs. There are several ways to do it - I’ll assign them in variables.tf, using a list of objects. This is what makes the most sense to me, to reduce code-repeat but ymmv. Let’s take a look:variable &quot;valid_azs&quot; { default = [ { &quot;az&quot; : &quot;us-east-1a&quot;, &quot;ip&quot; : &quot;10.0.0.0/20&quot; }, { &quot;az&quot; : &quot;us-east-1b&quot;, &quot;ip&quot; : &quot;10.0.16.0/20&quot; }, { &quot;az&quot; : &quot;us-east-1c&quot;, &quot;ip&quot; : &quot;10.0.32.0/20&quot; }]}variable &quot;invalid_azs&quot; { default = [ { &quot;az&quot; : &quot;us-east-1a&quot;, &quot;ip&quot; : &quot;100.64.0.0/19&quot; }, { &quot;az&quot; : &quot;us-east-1b&quot;, &quot;ip&quot; : &quot;100.64.32.0/19&quot; }, { &quot;az&quot; : &quot;us-east-1c&quot;, &quot;ip&quot; : &quot;100.64.64.0/19&quot; }]}We can (and will) loop on that.Let’s take a look at the subnets.resource &quot;aws_subnet&quot; &quot;valid-this&quot; { for_each = { for index, az in var.valid_azs : index =&amp;gt; az } vpc_id = aws_vpc.main_vpc.id cidr_block = each.value.ip availability_zone = each.value.az tags = { Name = &quot;eks-in-${each.value.az}&quot;, ManagedBy = &quot;Terraform&quot; }}resource &quot;aws_subnet&quot; &quot;invalid-this&quot; { for_each = { for index, az in var.invalid_azs : index =&amp;gt; az } vpc_id = aws_vpc.main_vpc.id cidr_block = each.value.ip availability_zone = each.value.az tags = { Name = &quot;invalid-in-${each.value.az}&quot;, ManagedBy = &quot;Terraform&quot; } depends_on = [ aws_vpc_ipv4_cidr_block_association.invalid_cidr ]}The only catch here is the for_each loop - it breaks down to a few steps: we grab the index on the main list with this index in hand, we can break the object in a key/value (that is, for each object in the list, break it in a key/value model” we use the valuesWe also need to explicitly tell Terraform to wait for the secondary CIDR to be created in the VPC (as, of course, we’re creating subnets with the IPs from such block).Once we have these, let’s create two new files, so our directory resembles this:➜ networking git:(main) ✗ tree .├── internetgateway.tf├── provider.tf├── routetables.tf├── subnet.tf├── variables.tf└── vpc.tfthe internetgateway.tf is the easiest one - we’ll simply allow our VPC to access the internet. This is the snippet:resource &quot;aws_internet_gateway&quot; &quot;this&quot; { vpc_id = aws_vpc.main_vpc.id tags = { Name = &quot;default-eks&quot; }}The routetables.tf is also quite simple:resource &quot;aws_route_table&quot; &quot;this&quot; { vpc_id = aws_vpc.main_vpc.id route { cidr_block = &quot;0.0.0.0/0&quot; gateway_id = aws_internet_gateway.this.id }}resource &quot;aws_route_table_association&quot; &quot;invalid&quot; { subnet_id = aws_subnet.invalid-this[count.index].id route_table_id = aws_route_table.this.id count = 3}resource &quot;aws_route_table_association&quot; &quot;valid&quot; { subnet_id = aws_subnet.valid-this[count.index].id route_table_id = aws_route_table.this.id count = 3}A Route Table defines how our EC2 instances can communicate with the internet (somewhat like a router-firewall). In this case, we’re being quite broad - and allowing all communication (look at the 0.0.0.0).The Route Table Association is associating the subnets with the current route table. We’re using count to perform a loop in every subnet we’re creating.If everything is right - you can now run terraform fmt -recursive to format your code and run terraform plan.The result should be:Plan: 16 to add, 0 to change, 0 to destroy.Thanks for tuning in so far. You can find this code in my github repo" }, { "title": "External Secrets Operator", "url": "/posts/external-secrets-operator/", "categories": "Kubernetes", "tags": "guide", "date": "2022-01-25 08:00:00 -0300", "snippet": "External Secrets OperatorExternal Secrets Operator is a tool to improve the security of your kubernetes secrets, by using a secure secret manager (i.e, AWS, Vault, etc) and automatically refreshing the data in your cluster.According to their own documentation: The goal of External Secrets Operator is to synchronize secrets from external APIs into Kubernetes. ESO is a collection of custom API resources - ExternalSecret, SecretStore and ClusterSecretStore that provide a user-friendly abstraction for the external API that stores and manages the lifecycle of the secrets for you.Basic components: ExternalSecret:This CRD main function is to provide which secret we want to grab from the remote secret manager. If it all behaves correctly, it’ll create an Opaque Secret in the cluster with the requested data. SecretStore:This CRD main function is to create a connection with the remote secret manager. It also refreshes the state of such secret and update the generated Opaque Secret ClusterSecretStore:This CRD main function is the same as the SecretStore. However, this should be used when multi-namespaces are required. That is, you have a cluster with several applications segregated by namespaces and want to have only one secret store. Hands on:We’ll create a Secret Store, which can only be interacted in the given namespace. If you happen to require a multi-namespace secret store (i.e, multiple namespaces using the same Secret Store), one can use the Cluster Secret StoreFirst of all, let’s setup the CRDs using Helm. Here’s my current kubernetes cluster:➜ bernardolopes.me git:(main) ✗ k get all --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system pod/calico-kube-controllers-7cd8958654-rg4mb 1/1 Running 12 (3d10h ago) 5dkube-system pod/calico-node-ddmz9 1/1 Running 12 (3d10h ago) 5dingress pod/nginx-ingress-microk8s-controller-kbrbj 1/1 Running 0 2d22hNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault service/kubernetes ClusterIP 10.152.183.1 &amp;lt;none&amp;gt; 443/TCP 5dNAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEkube-system daemonset.apps/calico-node 1 1 1 1 1 kubernetes.io/os=linux 5dingress daemonset.apps/nginx-ingress-microk8s-controller 1 1 1 1 1 &amp;lt;none&amp;gt; 2d22hNAMESPACE NAME READY UP-TO-DATE AVAILABLE AGEkube-system deployment.apps/calico-kube-controllers 1/1 1 1 5dsystem deployment.apps/controller-manager 0/1 0 0 2d21hNAMESPACE NAME DESIRED CURRENT READY AGEkube-system replicaset.apps/calico-kube-controllers-6966456d6b 0 0 0 5dkube-system replicaset.apps/calico-kube-controllers-7cd8958654 1 1 1 5dsystem replicaset.apps/controller-manager-7bfc98cf47 1 0 0 2d21hThat’s a bog standard microk8s cluster with Ingress enabled.To install it, follow their documentation. The result should be:NAME: external-secretsLAST DEPLOYED: Tue Jan 25 09:45:59 2022NAMESPACE: external-secretsSTATUS: deployedREVISION: 1TEST SUITE: NoneNOTES:external-secrets has been deployed successfully!In order to begin using ExternalSecrets, you will need to set up a SecretStoreor ClusterSecretStore resource (for example, by creating a &#39;vault&#39; SecretStore).More information on the different types of SecretStores and how to configure themcan be found in our Github: https://github.com/external-secrets/external-secretsWe’re almost there!I have set up a small vault application for this use case. You can have a Vault project running in under 10 minutes.If you choose to use this provider, consider creating a set of secrets such as the official documentationLet’s create a SecretStore now. Here’s my secret-store.yaml:➜ cat secret-store.yaml apiVersion: external-secrets.io/v1alpha1kind: SecretStoremetadata: name: vault-backendspec: provider: vault: server: &quot;http://10.123.228.81:8200&quot; path: &quot;secret&quot; version: &quot;v1&quot; auth: tokenSecretRef: name: &quot;vault-token&quot; namespace: &quot;default&quot; key: &quot;token&quot;---apiVersion: v1kind: Secretmetadata: name: vault-tokendata: token: cy5zUUtPTHVoUG84YzhXdVZPNzRBaE1WWjIK # the token encoded in base64Save this to a .yaml file, edit to match your server and token configuration. Then, run a:kubectl apply -f secret-store.yaml -n external-secretsNow, let’s create a ExternalSecret:apiVersion: external-secrets.io/v1alpha1kind: ExternalSecretmetadata: name: vault-examplespec: refreshInterval: &quot;15s&quot; secretStoreRef: name: vault-backend kind: SecretStore target: name: example-sync dataFrom: - key: fooAfter applying all of them, we should have:➜ external-secrets k get externalsecrets NAME STORE REFRESH INTERVAL STATUSexternalsecret.external-secrets.io/vault-example vault-backend 15s SecretSyncedNAME AGEsecretstore.external-secrets.io/vault-backend 25mWe can grab the secret data by either attaching it to a pod/deploy/svc or by running:➜ external-secrets k get secret example-sync -ojsonand here we go:{ &quot;apiVersion&quot;: &quot;v1&quot;, &quot;data&quot;: { &quot;my-value&quot;: &quot;czNjcjN0&quot; }, [...] Per default, this secret is encoded in base64, so we can run **echo czNjcjN0 base64 -d**: ➜ external-secrets echo czNjcjN0 | base64 -ds3cr3t% Thanks for tuning in so far! :-)" } ]
